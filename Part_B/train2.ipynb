{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Nature12KDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=\"../../inaturalist_12K\", batch_size=64, image_size=(512, 512), data_aug=False):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.data_aug = data_aug\n",
    "\n",
    "    @staticmethod\n",
    "    def get_transform(image_size, data_aug=False):\n",
    "        transform_list = [transforms.Resize(image_size)]\n",
    "\n",
    "        if data_aug:\n",
    "            transform_list += [\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
    "            ]\n",
    "\n",
    "        transform_list += [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "\n",
    "        return transforms.Compose(transform_list)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        train_transform = self.get_transform(self.image_size, self.data_aug)\n",
    "        test_transform = self.get_transform(self.image_size, False)\n",
    "\n",
    "        # Load train and test datasets\n",
    "        full_train = datasets.ImageFolder(os.path.join(self.data_dir, \"train\"), transform=train_transform)\n",
    "        test_set = datasets.ImageFolder(os.path.join(self.data_dir, \"test\"), transform=test_transform)\n",
    "\n",
    "        # Split train into train + val (80-20 split)\n",
    "        val_size = int(0.2 * len(full_train))\n",
    "        train_size = len(full_train) - val_size\n",
    "        self.train_set, self.val_set = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "        self.test_set = test_set\n",
    "        self.class_names = full_train.classes\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "class ResNetFinetune(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=1e-3, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Load pre-trained ResNet50\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Optionally freeze all backbone layers\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Replace the classifier head\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"test_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    data_module = Nature12KDataModule(\n",
    "        data_dir=\"../../inaturalist_12K\",  # ✅ Correct path here\n",
    "        batch_size=config.batch_size,\n",
    "        image_size=(256, 256),\n",
    "        data_aug=config.data_augmentation\n",
    "    )\n",
    "\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup()\n",
    "\n",
    "    model = ResNetFinetune(\n",
    "    num_classes=len(data_module.class_names),\n",
    "    lr=config.lr,\n",
    "    freeze_backbone=config.freeze_backbone\n",
    ")\n",
    "\n",
    "\n",
    "    wandb_logger = WandbLogger(project=wandb.run.project, name=wandb.run.name)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        log_every_n_steps=10,\n",
    "        logger=wandb_logger  # ✅ Correct logger\n",
    "    )\n",
    "\n",
    "    print(\"🚀 Training model...\")\n",
    "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
    "\n",
    "    # print(\"🧪 Evaluating on test set...\")\n",
    "    # trainer.test(model, data_module.test_dataloader())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_sweep():\n",
    "    sweep_config = {\n",
    "        'method': 'bayes',\n",
    "        'metric': {\n",
    "            'name': 'val_acc',\n",
    "            'goal': 'maximize'\n",
    "        },\n",
    "        'parameters': {\n",
    "            'lr': {\n",
    "                'min': 1e-5,\n",
    "                'max': 1e-2\n",
    "            },\n",
    "            'batch_size': {\n",
    "                'values': [16]\n",
    "            },\n",
    "            'freeze_backbone': {\n",
    "                'values': [True, False]\n",
    "            },\n",
    "            'data_augmentation': {\n",
    "                'values': [True, False]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project='iNaturalist_ResNet_Sweep')\n",
    "    wandb.agent(sweep_id, function=train, count=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: wtlpihq4\n",
      "Sweep URL: https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1020zjfl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.004214525949638218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24s019\u001b[0m (\u001b[33mda24s019-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_125346-1020zjfl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1020zjfl' target=\"_blank\">fallen-sweep-1</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1020zjfl' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1020zjfl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1285d680db214044bc6bebb79b0c872b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c972427f92d244c098a2dac15888431e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3ec7a41c77400cbc953875e6dd9b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301b3e11f3324d7994bbbdd1e4ed8479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0112992e26b94bc0813b281a05c16c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c8beeb987a4384b270e1801adde90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979f0a88824f48b59881af508b4fcdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆█████████</td></tr><tr><td>train_acc</td><td>▅▃▃▂▅▆▃▅▆▅▆▅█▇▇▅▆▁█▂▆▅▄▆▆▅▂▄▅▂▅▄▅▇▇▆▆▅▄▅</td></tr><tr><td>train_loss</td><td>█▆▆▃▃▂▃▄▂▅▅▂▄▁▂▂▃▄▄▃▂▅▂▆▃▃▃▁▃▃▄▃▃▂▁▂▄▃▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▆█▇▅</td></tr><tr><td>val_loss</td><td>▁▅▂▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_acc</td><td>0.5625</td></tr><tr><td>train_loss</td><td>1.3486</td></tr><tr><td>trainer/global_step</td><td>2499</td></tr><tr><td>val_acc</td><td>0.69135</td></tr><tr><td>val_loss</td><td>1.35735</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fallen-sweep-1</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1020zjfl' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1020zjfl</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_125346-1020zjfl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4kcnywvg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0017426635190226413\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_132058-4kcnywvg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4kcnywvg' target=\"_blank\">happy-sweep-2</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4kcnywvg' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4kcnywvg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b032634771a4b75ba0d5a7d6f8d8838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc942c03f3440be8d508b083cf066eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b1015e8e214d88b9d122e02637d670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2eb17af0cc1470e81738ecf12b0ae6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7f8e12e65d45fc854329c02a515584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e10a2f16b194296b91d53f9a3de6b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d6de8df34548469b027462481ee518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆███████████</td></tr><tr><td>train_acc</td><td>▁▂▆▅▆▃▄▄▆▄▆▅▅▆▆▇▃▃▆▆▇▅▆▇▆▄▆▅▅▂▅▆▃█▅▃▄▆▆▆</td></tr><tr><td>train_loss</td><td>█▃▆▅▃▃▂▅▃▄▃▂▆▄▄▁▄▇▇▄▄▇▄▄▄▄▁▅▂▃▁▃▂▆▆▅▅▁▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>val_acc</td><td>▂▆▁▆█</td></tr><tr><td>val_loss</td><td>▃▁█▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_acc</td><td>0.625</td></tr><tr><td>train_loss</td><td>1.04671</td></tr><tr><td>trainer/global_step</td><td>2499</td></tr><tr><td>val_acc</td><td>0.72736</td></tr><tr><td>val_loss</td><td>0.93253</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-sweep-2</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4kcnywvg' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4kcnywvg</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_132058-4kcnywvg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3t03tswg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0003882031389954307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_134822-3t03tswg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/3t03tswg' target=\"_blank\">glorious-sweep-3</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/3t03tswg' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/3t03tswg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae8e852e7434dfa8667873f2b44e6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3190544102954c43a251efb31d193f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758990069e4b4962a397277f96a49683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1b1ee4d8cf488bbeed606eca7a3197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64db944c0c4f4e9187542f513d3ad030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6122a70f776d42df92e8fd938a4bae2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ee3759e2374422a40b82f28ad05d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆███████</td></tr><tr><td>train_acc</td><td>▁▄▂▆▄▄▄▅▇▇▆▄▄▇▇▇█▅▆▆▅▅▅▆▆▆▅▆▇▆▅▃▆▅▄▇▆▅▇▇</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▄▄▄▄▃▄▃▄▂▂▂▄▄▄▃▃▂▃▄▄▂▂▂▂▃▂▃▂▁▃▂▂▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▃▅█▇</td></tr><tr><td>val_loss</td><td>█▄▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_acc</td><td>0.625</td></tr><tr><td>train_loss</td><td>1.03019</td></tr><tr><td>trainer/global_step</td><td>2499</td></tr><tr><td>val_acc</td><td>0.72486</td></tr><tr><td>val_loss</td><td>0.82688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glorious-sweep-3</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/3t03tswg' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/3t03tswg</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_134822-3t03tswg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4hxozurm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0007883123628094248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141547-4hxozurm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4hxozurm' target=\"_blank\">soft-sweep-4</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4hxozurm' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4hxozurm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bfb0b04bcc4c85b5a8420c88ba18c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aff61fe0afa4bf994eb25ca34c29b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-sweep-4</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4hxozurm' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4hxozurm</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141547-4hxozurm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 4hxozurm errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1056, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 150, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 320, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 192, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 1302, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py\", line 154, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 239, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 493, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py\", line 223, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 146, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 140, in closure\n",
      "    self._backward_fn(step_output.closure_loss)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 241, in backward_fn\n",
      "    call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 213, in backward\n",
      "    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 73, in backward\n",
      "    model.backward(tensor, *args, **kwargs)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 1097, in backward\n",
      "    loss.backward(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/_tensor.py\", line 626, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 21.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4hxozurm errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1056, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.fit_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.epoch_loop.run(self._data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 150, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance(data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 320, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 192, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._optimizer_step(batch_idx, closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 1302, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py\", line 154, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 239, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return optimizer.step(closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 493, in wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     out = func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ret = func(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py\", line 223, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     closure_result = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                      ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 146, in __call__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._result = self.closure(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 140, in closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._backward_fn(step_output.closure_loss)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 241, in backward_fn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 213, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 73, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model.backward(tensor, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 1097, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss.backward(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/_tensor.py\", line 626, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.autograd.backward(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     _engine_run_backward(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 21.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sw6nzsf3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008409946237074702\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141559-sw6nzsf3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/sw6nzsf3' target=\"_blank\">mild-sweep-5</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/sw6nzsf3' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/sw6nzsf3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a9c7bc8d1642d78169b0b9357961fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-sweep-5</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/sw6nzsf3' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/sw6nzsf3</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141559-sw6nzsf3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run sw6nzsf3 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5531/881287421.py\", line 36, in validation_step\n",
      "    logits = self(x)\n",
      "             ^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5531/881287421.py\", line 23, in forward\n",
      "    return self.backbone(x)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "    return self._forward_impl(x)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 268, in _forward_impl\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run sw6nzsf3 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/881287421.py\", line 36, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     logits = self(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/881287421.py\", line 23, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.backbone(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._forward_impl(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 268, in _forward_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5pysgyi8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0005297559160868275\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141616-5pysgyi8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5pysgyi8' target=\"_blank\">vocal-sweep-6</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5pysgyi8' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5pysgyi8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f0b790eb3a4e4b8bcd991fc9c4f998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-6</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5pysgyi8' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5pysgyi8</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141616-5pysgyi8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 5pysgyi8 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5pysgyi8 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5xgtcet5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0007793180385786153\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.9s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141631-5xgtcet5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5xgtcet5' target=\"_blank\">radiant-sweep-7</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5xgtcet5' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5xgtcet5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dda6e826ba8489abccb92f7814b98a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-sweep-7</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5xgtcet5' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5xgtcet5</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141631-5xgtcet5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 5xgtcet5 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5xgtcet5 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q9yc7j5u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.003784593725500293\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.9s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141652-q9yc7j5u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/q9yc7j5u' target=\"_blank\">revived-sweep-8</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/q9yc7j5u' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/q9yc7j5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae5f31edf134170bdaf3047b9f435e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-sweep-8</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/q9yc7j5u' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/q9yc7j5u</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141652-q9yc7j5u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run q9yc7j5u errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run q9yc7j5u errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tyuh9ceo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.007092011779760475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (1.4s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141705-tyuh9ceo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/tyuh9ceo' target=\"_blank\">firm-sweep-9</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/tyuh9ceo' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/tyuh9ceo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93f386797054d8fb9c829b5369719f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-sweep-9</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/tyuh9ceo' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/tyuh9ceo</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141705-tyuh9ceo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run tyuh9ceo errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run tyuh9ceo errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gdudqsf0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.004512825368464417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141717-gdudqsf0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/gdudqsf0' target=\"_blank\">fast-sweep-10</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/gdudqsf0' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/gdudqsf0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488ad6870f7947e4a08b6ef67d09413a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-sweep-10</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/gdudqsf0' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/gdudqsf0</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141717-gdudqsf0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run gdudqsf0 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run gdudqsf0 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ecu8mrdi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.006420313309829079\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141729-ecu8mrdi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/ecu8mrdi' target=\"_blank\">absurd-sweep-11</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/ecu8mrdi' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/ecu8mrdi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c182e06f4a44a9a00fce1608c11ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-sweep-11</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/ecu8mrdi' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/ecu8mrdi</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141729-ecu8mrdi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ecu8mrdi errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ecu8mrdi errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1c6w5f7e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.003679178022242919\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141739-1c6w5f7e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1c6w5f7e' target=\"_blank\">northern-sweep-12</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1c6w5f7e' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1c6w5f7e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876bcf94931e4659a8ffd606a2cd797b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-sweep-12</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1c6w5f7e' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/1c6w5f7e</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141739-1c6w5f7e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1c6w5f7e errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 1c6w5f7e errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pptuiytg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0014365627429873986\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141755-pptuiytg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/pptuiytg' target=\"_blank\">upbeat-sweep-13</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/pptuiytg' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/pptuiytg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa242e07bf1496d8c1e18cc2c715af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-sweep-13</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/pptuiytg' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/pptuiytg</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141755-pptuiytg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run pptuiytg errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run pptuiytg errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 80850tuz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00979849512612418\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141811-80850tuz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/80850tuz' target=\"_blank\">proud-sweep-14</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/80850tuz' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/80850tuz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f9cb48782245e9b78db853634e17d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">proud-sweep-14</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/80850tuz' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/80850tuz</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141811-80850tuz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 80850tuz errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 80850tuz errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 6.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.87 GiB is allocated by PyTorch, and 25.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2239w9pp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.007054351351766252\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141817-2239w9pp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/2239w9pp' target=\"_blank\">upbeat-sweep-15</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/2239w9pp' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/2239w9pp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7ddbbcf5da4ef68e32c9eb24d8ec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-sweep-15</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/2239w9pp' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/2239w9pp</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141817-2239w9pp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2239w9pp errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5531/881287421.py\", line 36, in validation_step\n",
      "    logits = self(x)\n",
      "             ^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5531/881287421.py\", line 23, in forward\n",
      "    return self.backbone(x)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "    return self._forward_impl(x)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 269, in _forward_impl\n",
      "    x = self.bn1(x)\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py\", line 2822, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 1.92 GiB memory in use. Of the allocated memory 1.85 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2239w9pp errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/881287421.py\", line 36, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     logits = self(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/881287421.py\", line 23, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.backbone(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._forward_impl(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 269, in _forward_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.bn1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.batch_norm(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py\", line 2822, in batch_norm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.batch_norm(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 24.06 MiB is free. Including non-PyTorch memory, this process has 1.92 GiB memory in use. Of the allocated memory 1.85 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4tgh8qjw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001485530774074294\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141833-4tgh8qjw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4tgh8qjw' target=\"_blank\">leafy-sweep-16</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4tgh8qjw' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4tgh8qjw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d6f05b8a4641e09ed0ee2f2ef57935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-sweep-16</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4tgh8qjw' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4tgh8qjw</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141833-4tgh8qjw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 4tgh8qjw errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5531/881287421.py\", line 36, in validation_step\n",
      "    logits = self(x)\n",
      "             ^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5531/881287421.py\", line 23, in forward\n",
      "    return self.backbone(x)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "    return self._forward_impl(x)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 268, in _forward_impl\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4tgh8qjw errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/881287421.py\", line 36, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     logits = self(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/881287421.py\", line 23, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.backbone(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._forward_impl(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 268, in _forward_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4sd3bven with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0019360576553256231\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141839-4sd3bven</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4sd3bven' target=\"_blank\">blooming-sweep-17</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4sd3bven' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4sd3bven</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bbfa730ce74fc6bef37ab66ec2bdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-sweep-17</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4sd3bven' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/4sd3bven</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141839-4sd3bven/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 4sd3bven errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4sd3bven errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5vc4gro9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00033285719207458913\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141854-5vc4gro9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5vc4gro9' target=\"_blank\">driven-sweep-18</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5vc4gro9' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5vc4gro9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ea9fb167804b049f624f3d0191e4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">driven-sweep-18</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5vc4gro9' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/5vc4gro9</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141854-5vc4gro9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 5vc4gro9 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5vc4gro9 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: esbycz6y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008999639132720448\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141901-esbycz6y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/esbycz6y' target=\"_blank\">northern-sweep-19</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/esbycz6y' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/esbycz6y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18f90bcfa5d432ba61fbe9c6e014c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-sweep-19</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/esbycz6y' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/esbycz6y</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141901-esbycz6y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run esbycz6y errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run esbycz6y errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8ruibtba with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.003509313859165659\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141916-8ruibtba</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/8ruibtba' target=\"_blank\">winter-sweep-20</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/8ruibtba' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/8ruibtba</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775adb48c4404972b59ad6588550be41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-20</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/8ruibtba' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/8ruibtba</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141916-8ruibtba/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 8ruibtba errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 8ruibtba errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0t7wu00q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001433800612992729\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141923-0t7wu00q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/0t7wu00q' target=\"_blank\">radiant-sweep-21</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/0t7wu00q' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/0t7wu00q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e58f41bfb424e99b7315e277c6078a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-sweep-21</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/0t7wu00q' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/0t7wu00q</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141923-0t7wu00q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 0t7wu00q errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 0t7wu00q errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eqkjdcqw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.009555748298617469\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141939-eqkjdcqw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/eqkjdcqw' target=\"_blank\">dry-sweep-22</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/eqkjdcqw' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/eqkjdcqw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f50f9668714294960d5fb22dff11c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-sweep-22</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/eqkjdcqw' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/eqkjdcqw</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141939-eqkjdcqw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run eqkjdcqw errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run eqkjdcqw errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 031e8b6y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.002763140151253975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.9s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_141955-031e8b6y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/031e8b6y' target=\"_blank\">toasty-sweep-23</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/031e8b6y' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/031e8b6y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1625413d502d4771b41a5157dbe9db7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-sweep-23</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/031e8b6y' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/031e8b6y</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_141955-031e8b6y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 031e8b6y errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 031e8b6y errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h7re90dm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.005139881733127954\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_142007-h7re90dm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/h7re90dm' target=\"_blank\">pious-sweep-24</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/h7re90dm' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/h7re90dm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a63f0a5129f4165a8096a52ff4702a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pious-sweep-24</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/h7re90dm' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/h7re90dm</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_142007-h7re90dm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run h7re90dm errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run h7re90dm errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mc7nqwrp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.004773176723429989\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_142018-mc7nqwrp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/mc7nqwrp' target=\"_blank\">brisk-sweep-25</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/mc7nqwrp' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/mc7nqwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d48b9169b84875acdbc33d9dd24648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-sweep-25</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/mc7nqwrp' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/mc7nqwrp</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_142018-mc7nqwrp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run mc7nqwrp errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run mc7nqwrp errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s7r4gwgm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.00878443216016242\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_142034-s7r4gwgm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/s7r4gwgm' target=\"_blank\">smooth-sweep-26</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/s7r4gwgm' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/s7r4gwgm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f678c0d25e4b19afc5279f83fd575b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-26</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/s7r4gwgm' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/s7r4gwgm</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_142034-s7r4gwgm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run s7r4gwgm errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run s7r4gwgm errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 376y17f2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0021944212368131253\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_142040-376y17f2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/376y17f2' target=\"_blank\">rosy-sweep-27</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/376y17f2' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/376y17f2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167539c2941c43e1b449bf00b8683654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-sweep-27</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/376y17f2' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/376y17f2</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_142040-376y17f2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 376y17f2 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 376y17f2 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x5wgcb5f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008067826830943804\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_142051-x5wgcb5f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/x5wgcb5f' target=\"_blank\">azure-sweep-28</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/x5wgcb5f' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/x5wgcb5f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c42ca6eeb6e4002ae97fec7676376bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">azure-sweep-28</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/x5wgcb5f' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/x5wgcb5f</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_142051-x5wgcb5f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run x5wgcb5f errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run x5wgcb5f errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wukckhp0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.008774679879763232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_142102-wukckhp0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/wukckhp0' target=\"_blank\">kind-sweep-29</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/wukckhp0' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/wukckhp0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "20.5 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd6ec6cf6b0490897736f7851bc16ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">kind-sweep-29</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/wukckhp0' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/wukckhp0</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_142102-wukckhp0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run wukckhp0 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wukckhp0 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f7pvlk96 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_backbone: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.004708952779903105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Documents/Books/DA24S019_Assignment_2/Part_B/wandb/run-20250414_142118-f7pvlk96</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/f7pvlk96' target=\"_blank\">easy-sweep-30</a></strong> to <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/sweeps/wtlpihq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/f7pvlk96' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/f7pvlk96</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 23.5 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.114    Total estimated model params size (MB)\n",
      "152       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'freeze_backbone' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46a020a37cf4d7d99ca73340fac889e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">easy-sweep-30</strong> at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/f7pvlk96' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep/runs/f7pvlk96</a><br> View project at: <a href='https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep' target=\"_blank\">https://wandb.ai/da24s019-indian-institute-of-technology-madras/iNaturalist_ResNet_Sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_142118-f7pvlk96/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run f7pvlk96 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "    trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run f7pvlk96 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_5531/3154369143.py\", line 33, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data_module.train_dataloader(), data_module.val_dataloader())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/pytorch_lightning/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/user/anaconda3/lib/python3.12/site-packages/lightning_fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 12.06 MiB is free. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.86 GiB is allocated by PyTorch, and 22.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "launch_sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
